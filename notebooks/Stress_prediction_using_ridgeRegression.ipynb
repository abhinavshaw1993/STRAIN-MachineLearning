{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run data_getter_and_processor.ipynb\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import display\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Obtain Training and testing data.\n",
    "train_x, test_x, train_y, test_y = get_split_train_data(random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2\n",
       "1    2\n",
       "2    2\n",
       "3    2\n",
       "4    2\n",
       "Name: worst_stress_level, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 1.0, 'normalize': True} -3.03298827367 Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
      "   normalize=True, random_state=None, solver='auto', tol=0.001)\n",
      "[ 2.9679184   0.93437035  2.09344308  2.0963043   1.6120607   3.11110459\n",
      "  1.66927616  1.42934325  2.54774115  1.91250764  1.51095754  2.2074159\n",
      "  1.30299789  1.48190942  0.8842465   1.35921274  3.28809704  2.06026197\n",
      "  1.216133    1.30367661  1.34861986  0.95247456  1.42956462  1.19321114\n",
      "  1.21022021  2.0144164   1.26137497  1.6391965   2.14453322  1.89720382\n",
      "  2.84229329  1.94615537  1.98715497  2.21215096  2.11746417  1.36722261\n",
      "  1.50228618  1.89629554  2.49372132  1.80713176  1.44196595  2.19456427\n",
      "  1.60086652  1.19914868  1.57965597  1.40333224  1.02878783  1.280647\n",
      "  1.91947418  1.60317008  2.1254349   1.29113542  2.2245023   1.68200515\n",
      "  1.97724616  1.90246788  1.41601123  1.5939767   2.29176614  2.50060329\n",
      "  2.6543492   2.58801066  2.27777282  1.9863463   1.74426227  1.65526794\n",
      "  2.31186502  3.44180548  2.0929764   2.52913171  2.49274057  3.08386166\n",
      "  1.65934388  1.13240291  2.13813231  1.83468909  3.43988364  2.3342043\n",
      "  1.14829869  1.84929069  0.6101305   1.52567492  1.88670589  1.44820266\n",
      "  2.73737739  1.7692381   2.48784658  2.74690057  2.09481541  2.43060626\n",
      "  2.22181915  1.22051633  2.32642313  1.65125433  2.29397829  1.01280914\n",
      "  0.62973208  2.04363979  1.66632149  2.08700832  0.99230085  1.45861787\n",
      "  3.75821518  2.1503404   1.45801477  2.62546507  2.55024175  2.01258512\n",
      "  1.96460354  1.94768613  1.84239784  1.05400884  3.33513915  2.13727125\n",
      "  1.52012344  1.95861583  1.72451082  2.71521635  1.93277394  2.12245533\n",
      "  1.76933435  2.43793549  2.3838855   2.24203173  1.69946081  2.69190092\n",
      "  1.54268191  1.80868222  1.71855777  1.39079257  1.7348659   2.26900366\n",
      "  1.90759314  1.91731764  2.0125047   2.35322588  2.81597084  1.57903822\n",
      "  2.33437537  2.03868634  1.79707613  2.45034811  1.3355833   1.54985569\n",
      "  1.86726328  2.02289211  1.910419    2.90051188  2.25141932  1.74689624\n",
      "  2.55619719  2.16992521  1.57792821  2.5516909   1.73831843  1.3235855\n",
      "  1.8422628   1.85074204  0.65759081  2.12553812  1.81175166  2.99636749\n",
      "  2.2363186   2.39128672  3.36929262  1.82942838  3.08195332  2.56971609\n",
      "  1.69170781  1.73494296  1.46714123  1.97312329  1.59208047  1.85085055\n",
      "  2.47302723  1.02589908  1.22919466  1.24080747  2.59500116  3.3723768\n",
      "  1.5939767   2.08541222  1.76666064  1.30645809  1.55489494  1.7439363\n",
      "  3.22920142  1.97334351  1.57484961  1.87977458  2.91335049  1.80117299\n",
      "  1.64892404  1.50601247  1.55730969  1.43195056  2.38312915  2.02792274\n",
      "  1.76832761  2.54081232  1.86298001  1.70519539  1.98477132  1.87056056\n",
      "  1.58440745  1.26138099  1.86493316  1.90735453  1.2501615   2.22262073\n",
      "  1.63874225  2.22342365  1.28565139  2.39887157  1.48243339  2.11453978\n",
      "  1.4863245   1.55778318  2.17529936  2.18327674  2.39395724  1.89161447\n",
      "  1.57966556  1.79883206  2.3071188   1.25164462  1.89967168  1.90000649\n",
      "  1.87713831  2.68725347  2.52542178  1.73121911  2.52391567  1.49006021\n",
      "  2.16431382  1.72027169  1.23784862  2.00783685  1.89145658  1.94272397\n",
      "  2.20021237  1.86378132  2.122409    1.82121254  1.89021517  1.81033028\n",
      "  1.96082899  1.33836658  1.15156023  1.2553391   1.54690133  2.02563751\n",
      "  2.31600256  2.04797774  2.0916187   2.1895858   1.68862863  1.53347166\n",
      "  2.30924662  1.87881127  1.35801411  2.58783621  2.30073778  1.50819879\n",
      "  1.15693091  1.45236713  1.93127862  1.91000663  1.25778576  2.14470042\n",
      "  2.99895345  2.78303699  2.0534588   1.72295606  1.76382017  1.76054344\n",
      "  1.56366883  2.08629106  2.0666669   2.4957309   2.80203082  1.31001093\n",
      "  1.79035791  1.81533742  2.68984471  2.38011688  1.79475352  1.97681462\n",
      "  2.15558145  1.93000026  2.69944965  1.9183606   2.36538307  1.90244587\n",
      "  2.06735192  1.85918189  1.52465113  3.09870915  1.96464231  2.19840204\n",
      "  2.14030112  2.24936131  2.5643138   1.93907454  2.8706274   1.58589188\n",
      "  1.48690819  1.8920241   2.39862414  1.43222926  1.45527754  1.47673403\n",
      "  1.48050977  1.76322852  1.71958996  1.64340937  2.28769075  2.5251809\n",
      "  1.7851559   1.40673199  2.64908144  2.81016495  1.73363726  2.07517124\n",
      "  2.60162843  1.7132654   1.85054233  1.84105954  2.5916215   1.87387833\n",
      "  1.7789842   2.72299607  1.37731818  1.95195169  2.66989407  1.35971855\n",
      "  2.68817493  1.97872223  2.43255878  1.42320379  2.64738764  1.55975253\n",
      "  2.45278385  1.77700241  2.27050564  2.00217025  2.38175133  2.13747304\n",
      "  1.93669304  2.43163007  2.13862175  3.11096226  1.98168213  1.80402905\n",
      "  2.9688874   1.39409489  2.41748159  2.48279369  0.94763606  1.78970177\n",
      "  2.80697598  2.66106175  1.69365443  2.6047482   1.33801215  1.35984299\n",
      "  2.39806578  2.39818844  1.88162994  2.20089132  2.1423467   1.1047233\n",
      "  1.84655948  1.73098538  2.1415777   2.57766304  3.84591869  2.75646132\n",
      "  1.76647769  2.40636069  3.56220092  1.85679466  1.61320851  2.62505749\n",
      "  2.20363902  1.57992545  2.41349038  2.00889837]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Apply gridSearch to search best Random Forest model for imbalanced data for worst\n",
    "worst_stress_levels = train_y.loc[:,\"worst_stress_level\"]\n",
    "ballanced_train_x, worst_stress_levels = balance_data(train_x, worst_stress_levels) \n",
    "\n",
    "param_grid = [\n",
    "    {'alpha': [10e-3, 10e-4, 10e-5, 10e-2, 10e-1, 1, 10],\n",
    "    'normalize': [True]}\n",
    " ]\n",
    "\n",
    "display(worst_stress_levels.head())\n",
    "# print(worst_stress_levels.columns.values)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(Ridge(), param_grid, cv=5, scoring=\"neg_mean_squared_error\", n_jobs=-1)\n",
    "clf.fit(ballanced_train_x, worst_stress_levels)\n",
    "print(clf.best_params_, clf.best_score_, clf.best_estimator_)\n",
    "\n",
    "# predicting only worst stress levels.\n",
    "estimator = clf.estimator\n",
    "estimator.fit(ballanced_train_x, worst_stress_levels)\n",
    "pred_worst_stress_levels = estimator.predict(test_x)\n",
    "\n",
    "print(pred_worst_stress_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst stress levels accuracy is 35.3092783505 %\n",
      "[ 3.  1.  2.  2.  2.  3.  2.  1.  3.  2.  2.  2.  1.  1.  1.  1.  3.  2.\n",
      "  1.  1.  1.  1.  1.  1.  1.  2.  1.  2.  2.  2.  3.  2.  2.  2.  2.  1.\n",
      "  2.  2.  2.  2.  1.  2.  2.  1.  2.  1.  1.  1.  2.  2.  2.  1.  2.  2.\n",
      "  2.  2.  1.  2.  2.  3.  3.  3.  2.  2.  2.  2.  2.  3.  2.  3.  2.  3.\n",
      "  2.  1.  2.  2.  3.  2.  1.  2.  1.  2.  2.  1.  3.  2.  2.  3.  2.  2.\n",
      "  2.  1.  2.  2.  2.  1.  1.  2.  2.  2.  1.  1.  4.  2.  1.  3.  3.  2.\n",
      "  2.  2.  2.  1.  3.  2.  2.  2.  2.  3.  2.  2.  2.  2.  2.  2.  2.  3.\n",
      "  2.  2.  2.  1.  2.  2.  2.  2.  2.  2.  3.  2.  2.  2.  2.  2.  1.  2.\n",
      "  2.  2.  2.  3.  2.  2.  3.  2.  2.  3.  2.  1.  2.  2.  1.  2.  2.  3.\n",
      "  2.  2.  3.  2.  3.  3.  2.  2.  1.  2.  2.  2.  2.  1.  1.  1.  3.  3.\n",
      "  2.  2.  2.  1.  2.  2.  3.  2.  2.  2.  3.  2.  2.  2.  2.  1.  2.  2.\n",
      "  2.  3.  2.  2.  2.  2.  2.  1.  2.  2.  1.  2.  2.  2.  1.  2.  1.  2.\n",
      "  1.  2.  2.  2.  2.  2.  2.  2.  2.  1.  2.  2.  2.  3.  3.  2.  3.  1.\n",
      "  2.  2.  1.  2.  2.  2.  2.  2.  2.  2.  2.  2.  2.  1.  1.  1.  2.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  1.  3.  2.  2.  1.  1.  2.  2.  1.  2.\n",
      "  3.  3.  2.  2.  2.  2.  2.  2.  2.  2.  3.  1.  2.  2.  3.  2.  2.  2.\n",
      "  2.  2.  3.  2.  2.  2.  2.  2.  2.  3.  2.  2.  2.  2.  3.  2.  3.  2.\n",
      "  1.  2.  2.  1.  1.  1.  1.  2.  2.  2.  2.  3.  2.  1.  3.  3.  2.  2.\n",
      "  3.  2.  2.  2.  3.  2.  2.  3.  1.  2.  3.  1.  3.  2.  2.  1.  3.  2.\n",
      "  2.  2.  2.  2.  2.  2.  2.  2.  2.  3.  2.  2.  3.  1.  2.  2.  1.  2.\n",
      "  3.  3.  2.  3.  1.  1.  2.  2.  2.  2.  2.  1.  2.  2.  2.  3.  4.  3.\n",
      "  2.  2.  4.  2.  2.  3.  2.  2.  2.  2.]\n",
      "[ 2.97  0.93  2.09  2.1   1.61  3.11  1.67  1.43  2.55  1.91  1.51  2.21\n",
      "  1.3   1.48  0.88  1.36  3.29  2.06  1.22  1.3   1.35  0.95  1.43  1.19\n",
      "  1.21  2.01  1.26  1.64  2.14  1.9   2.84  1.95  1.99  2.21  2.12  1.37\n",
      "  1.5   1.9   2.49  1.81  1.44  2.19  1.6   1.2   1.58  1.4   1.03  1.28\n",
      "  1.92  1.6   2.13  1.29  2.22  1.68  1.98  1.9   1.42  1.59  2.29  2.5\n",
      "  2.65  2.59  2.28  1.99  1.74  1.66  2.31  3.44  2.09  2.53  2.49  3.08\n",
      "  1.66  1.13  2.14  1.83  3.44  2.33  1.15  1.85  0.61  1.53  1.89  1.45\n",
      "  2.74  1.77  2.49  2.75  2.09  2.43  2.22  1.22  2.33  1.65  2.29  1.01\n",
      "  0.63  2.04  1.67  2.09  0.99  1.46  3.76  2.15  1.46  2.63  2.55  2.01\n",
      "  1.96  1.95  1.84  1.05  3.34  2.14  1.52  1.96  1.72  2.72  1.93  2.12\n",
      "  1.77  2.44  2.38  2.24  1.7   2.69  1.54  1.81  1.72  1.39  1.73  2.27\n",
      "  1.91  1.92  2.01  2.35  2.82  1.58  2.33  2.04  1.8   2.45  1.34  1.55\n",
      "  1.87  2.02  1.91  2.9   2.25  1.75  2.56  2.17  1.58  2.55  1.74  1.32\n",
      "  1.84  1.85  0.66  2.13  1.81  3.    2.24  2.39  3.37  1.83  3.08  2.57\n",
      "  1.69  1.73  1.47  1.97  1.59  1.85  2.47  1.03  1.23  1.24  2.6   3.37\n",
      "  1.59  2.09  1.77  1.31  1.55  1.74  3.23  1.97  1.57  1.88  2.91  1.8\n",
      "  1.65  1.51  1.56  1.43  2.38  2.03  1.77  2.54  1.86  1.71  1.98  1.87\n",
      "  1.58  1.26  1.86  1.91  1.25  2.22  1.64  2.22  1.29  2.4   1.48  2.11\n",
      "  1.49  1.56  2.18  2.18  2.39  1.89  1.58  1.8   2.31  1.25  1.9   1.9\n",
      "  1.88  2.69  2.53  1.73  2.52  1.49  2.16  1.72  1.24  2.01  1.89  1.94\n",
      "  2.2   1.86  2.12  1.82  1.89  1.81  1.96  1.34  1.15  1.26  1.55  2.03\n",
      "  2.32  2.05  2.09  2.19  1.69  1.53  2.31  1.88  1.36  2.59  2.3   1.51\n",
      "  1.16  1.45  1.93  1.91  1.26  2.14  3.    2.78  2.05  1.72  1.76  1.76\n",
      "  1.56  2.09  2.07  2.5   2.8   1.31  1.79  1.82  2.69  2.38  1.79  1.98\n",
      "  2.16  1.93  2.7   1.92  2.37  1.9   2.07  1.86  1.52  3.1   1.96  2.2\n",
      "  2.14  2.25  2.56  1.94  2.87  1.59  1.49  1.89  2.4   1.43  1.46  1.48\n",
      "  1.48  1.76  1.72  1.64  2.29  2.53  1.79  1.41  2.65  2.81  1.73  2.08\n",
      "  2.6   1.71  1.85  1.84  2.59  1.87  1.78  2.72  1.38  1.95  2.67  1.36\n",
      "  2.69  1.98  2.43  1.42  2.65  1.56  2.45  1.78  2.27  2.    2.38  2.14\n",
      "  1.94  2.43  2.14  3.11  1.98  1.8   2.97  1.39  2.42  2.48  0.95  1.79\n",
      "  2.81  2.66  1.69  2.6   1.34  1.36  2.4   2.4   1.88  2.2   2.14  1.1\n",
      "  1.85  1.73  2.14  2.58  3.85  2.76  1.77  2.41  3.56  1.86  1.61  2.63\n",
      "  2.2   1.58  2.41  2.01]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scoring the metrics.\n",
    "\n",
    "score = accuracy_score(test_y.loc[:,\"worst_stress_level\"], np.round(pred_worst_stress_levels), normalize=True)\n",
    "print(\"Worst stress levels accuracy is \"+ str(score * 100) + \" %\")\n",
    "\n",
    "print( np.round(pred_worst_stress_levels))\n",
    "print( np.round(pred_worst_stress_levels, decimals=2))\n",
    "print( )\n",
    "\n",
    "\n",
    "\n",
    "# f1 = f1_score(test_y.iloc[:,0], pred_worst_stress_levels, average=None)\n",
    "# print(\"Worst stress levels f_1 score \", f1)\n",
    "# print(\"predicted values\", pred_worst_stress_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
